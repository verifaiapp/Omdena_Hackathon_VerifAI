{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrns1QdmxzTX"
      },
      "source": [
        "# Malicious URL Detection\n",
        "\n",
        "Project by:  \n",
        "[Jen Patrick Nataba](https://ph.linkedin.com/in/cytojen)  \n",
        "[John Ferry Lagman](https://ph.linkedin.com/in/thatjohnlagman)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JQacmY6yqgy",
        "outputId": "c3f0e735-9d0d-44e2-ccc6-a09a8b635a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:41.376344Z",
          "iopub.status.busy": "2024-12-23T04:07:41.376051Z",
          "iopub.status.idle": "2024-12-23T04:07:46.076673Z",
          "shell.execute_reply": "2024-12-23T04:07:46.075684Z",
          "shell.execute_reply.started": "2024-12-23T04:07:41.376314Z"
        },
        "id": "eSzNNZMexzTY",
        "outputId": "01e10b05-5aea-49ec-aa23-c80c9e04d6de",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid, fd = os.forkpty()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qL1ISnUxzTZ"
      },
      "source": [
        "# import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.077970Z",
          "iopub.status.busy": "2024-12-23T04:07:46.077721Z",
          "iopub.status.idle": "2024-12-23T04:07:46.082990Z",
          "shell.execute_reply": "2024-12-23T04:07:46.082085Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.077946Z"
        },
        "id": "qZdD3igFxzTZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beQOLiIAxzTZ"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.084131Z",
          "iopub.status.busy": "2024-12-23T04:07:46.083804Z",
          "iopub.status.idle": "2024-12-23T04:07:46.098933Z",
          "shell.execute_reply": "2024-12-23T04:07:46.098091Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.084103Z"
        },
        "id": "nHU_tMtfxzTZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "def load_data():\n",
        "    return load_dataset(\"kmack/Phishing_urls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.100019Z",
          "iopub.status.busy": "2024-12-23T04:07:46.099662Z",
          "iopub.status.idle": "2024-12-23T04:07:46.111589Z",
          "shell.execute_reply": "2024-12-23T04:07:46.110803Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.099934Z"
        },
        "id": "7d_vL6GsxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "def preprocess_data(dataset):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def preprocess(example):\n",
        "        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
        "\n",
        "    encoded_dataset = dataset.map(preprocess, batched=True, num_proc=4)\n",
        "    encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    return encoded_dataset, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.113007Z",
          "iopub.status.busy": "2024-12-23T04:07:46.112606Z",
          "iopub.status.idle": "2024-12-23T04:07:46.125346Z",
          "shell.execute_reply": "2024-12-23T04:07:46.124600Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.112974Z"
        },
        "id": "ZpAV3DqSxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# split into training, validation, and test sets\n",
        "def split_data(dataset):\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "    val_test_split = train_test_split['test'].train_test_split(test_size=0.25)\n",
        "\n",
        "    train_data = train_test_split['train']\n",
        "    val_data = val_test_split['train']\n",
        "    test_data = val_test_split['test']\n",
        "\n",
        "    return train_data, val_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPfapjGxzTa"
      },
      "source": [
        "# modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.127959Z",
          "iopub.status.busy": "2024-12-23T04:07:46.127741Z",
          "iopub.status.idle": "2024-12-23T04:07:46.137897Z",
          "shell.execute_reply": "2024-12-23T04:07:46.137129Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.127940Z"
        },
        "id": "j8U6dpzmxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# load distilbert model sequence classification\n",
        "def load_model(num_labels=2):\n",
        "    return DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=num_labels\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.146300Z",
          "iopub.status.busy": "2024-12-23T04:07:46.146088Z",
          "iopub.status.idle": "2024-12-23T04:07:46.161758Z",
          "shell.execute_reply": "2024-12-23T04:07:46.160956Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.146281Z"
        },
        "id": "rCFs0fYzxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# accuracy, precision, recall, and F1-score\n",
        "def create_training_args(output_dir, learning_rate=2e-5, batch_size=16, epochs=2, run_name=\"malicious_url_detector\"):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        logging_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_steps=50,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=1,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        run_name=run_name,\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_strategy=\"epoch\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.163037Z",
          "iopub.status.busy": "2024-12-23T04:07:46.162705Z",
          "iopub.status.idle": "2024-12-23T04:07:46.174611Z",
          "shell.execute_reply": "2024-12-23T04:07:46.173914Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.162988Z"
        },
        "id": "wEZsK7oVxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZNbMzlWxzTb"
      },
      "source": [
        "# hyperparemeter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.175417Z",
          "iopub.status.busy": "2024-12-23T04:07:46.175226Z",
          "iopub.status.idle": "2024-12-23T04:07:46.183683Z",
          "shell.execute_reply": "2024-12-23T04:07:46.182912Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.175400Z"
        },
        "id": "fPLF6cvuxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning for learning rate, batch size, and epochs\n",
        "def hyperparameter_tuning(train_data, val_data, tokenizer, learning_rates, batch_sizes):\n",
        "    best_params = {}\n",
        "    best_f1 = 0\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"Training with LR={lr}, BS={bs}, Epochs=2\")\n",
        "\n",
        "            model = load_model()\n",
        "            args = create_training_args(\n",
        "                output_dir=f\"/content/drive/MyDrive/omdena_hackathon/models/malicious_url_detector\",\n",
        "                learning_rate=lr,\n",
        "                batch_size=bs,\n",
        "                epochs=2,\n",
        "                run_name=f\"LR_{lr}_BS_{bs}_E_2\"\n",
        "            )\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=train_data,\n",
        "                eval_dataset=val_data,\n",
        "                tokenizer=tokenizer,\n",
        "                compute_metrics=compute_metrics\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "            metrics = trainer.evaluate(val_data)\n",
        "            f1 = metrics[\"eval_f1\"]\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_params = {\"learning_rate\": lr, \"batch_size\": bs}\n",
        "\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_BOsIBNxzTb"
      },
      "source": [
        "# performance function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.184657Z",
          "iopub.status.busy": "2024-12-23T04:07:46.184414Z",
          "iopub.status.idle": "2024-12-23T04:07:46.199379Z",
          "shell.execute_reply": "2024-12-23T04:07:46.198610Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.184639Z"
        },
        "id": "dlFHl_SVxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# check model performance\n",
        "def evaluate_model(trainer, test_data):\n",
        "    metrics = trainer.evaluate(test_data)\n",
        "    print(\"evaluation Metrics:\", metrics)\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOP_rC4cxzTb"
      },
      "source": [
        "# call the functions and run the codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3b4566e5f7dd422f952402d9750076ed",
            "55551a53650b4bdcaa90503f53c250e5",
            "3072825ff8d4436e881714c731960def",
            "3df96e6178ec4409a6e079b6101b579d",
            "189d59d36c4b40e8a39aa34083224659",
            "50c550cb4a6e4d9ea2e5a002a7bac492",
            "4d9ae48f8d144c87bd07888c9c298cbb",
            "75c004df55e0476c95d570b18e421678",
            "c0e748cdb0d945b7906e5f3117831f3f",
            "a4403d9283704a018d72fa1bdb05a78a",
            "a2ea81e17fb94da1bc79c7880c4437e5",
            "be0c4b6a16c8476b84649d6b7c8d71d0",
            "d0f417bbd89047f7a44cc3d6bce5174a",
            "dec28026f55e48cea45a78eb4d9f9835"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.200392Z",
          "iopub.status.busy": "2024-12-23T04:07:46.200176Z",
          "iopub.status.idle": "2024-12-23T04:09:42.452036Z",
          "shell.execute_reply": "2024-12-23T04:09:42.451030Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.200374Z"
        },
        "id": "spCagm_VxzTb",
        "outputId": "cc4cae33-37a7-41e5-abaf-7c94b491a912",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b4566e5f7dd422f952402d9750076ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/518 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55551a53650b4bdcaa90503f53c250e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(â€¦)-00000-of-00001-d8afc95a165ea87b.parquet:   0%|          | 0.00/25.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3072825ff8d4436e881714c731960def",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(â€¦)-00000-of-00001-4d6cbda5297196e7.parquet:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3df96e6178ec4409a6e079b6101b579d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(â€¦)-00000-of-00001-4e1abfe96aa382c2.parquet:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "189d59d36c4b40e8a39aa34083224659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/567056 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50c550cb4a6e4d9ea2e5a002a7bac492",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d9ae48f8d144c87bd07888c9c298cbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating valid split:   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75c004df55e0476c95d570b18e421678",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0e748cdb0d945b7906e5f3117831f3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4403d9283704a018d72fa1bdb05a78a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2ea81e17fb94da1bc79c7880c4437e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be0c4b6a16c8476b84649d6b7c8d71d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/567056 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0f417bbd89047f7a44cc3d6bce5174a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dec28026f55e48cea45a78eb4d9f9835",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_data()\n",
        "encoded_dataset, tokenizer = preprocess_data(dataset)\n",
        "train_data, val_data, test_data = split_data(encoded_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7dfc0d68588b4b75b3562b8c7fc5764f"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-23T04:09:42.453370Z",
          "iopub.status.busy": "2024-12-23T04:09:42.453116Z",
          "iopub.status.idle": "2024-12-23T09:05:47.571271Z",
          "shell.execute_reply": "2024-12-23T09:05:47.570339Z",
          "shell.execute_reply.started": "2024-12-23T04:09:42.453337Z"
        },
        "id": "OvwJ6GOTxzTb",
        "outputId": "480406fa-9118-4f1b-a859-81d318c4ad50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=2e-05, BS=16, Epochs=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dfc0d68588b4b75b3562b8c7fc5764f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28354' max='28354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28354/28354 1:19:56, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.257200</td>\n",
              "      <td>0.235134</td>\n",
              "      <td>0.896860</td>\n",
              "      <td>0.857865</td>\n",
              "      <td>0.949317</td>\n",
              "      <td>0.901277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.203900</td>\n",
              "      <td>0.234728</td>\n",
              "      <td>0.898647</td>\n",
              "      <td>0.857960</td>\n",
              "      <td>0.953489</td>\n",
              "      <td>0.903206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2659' max='2659' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2659/2659 02:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=2e-05, BS=32, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:43, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.230400</td>\n",
              "      <td>0.230629</td>\n",
              "      <td>0.896766</td>\n",
              "      <td>0.859242</td>\n",
              "      <td>0.946970</td>\n",
              "      <td>0.900975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.202100</td>\n",
              "      <td>0.226526</td>\n",
              "      <td>0.898941</td>\n",
              "      <td>0.858402</td>\n",
              "      <td>0.953513</td>\n",
              "      <td>0.903461</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1330/1330 01:53]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=3e-05, BS=16, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28354' max='28354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28354/28354 1:20:04, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.236194</td>\n",
              "      <td>0.896719</td>\n",
              "      <td>0.855801</td>\n",
              "      <td>0.952186</td>\n",
              "      <td>0.901424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.206200</td>\n",
              "      <td>0.238984</td>\n",
              "      <td>0.898153</td>\n",
              "      <td>0.857557</td>\n",
              "      <td>0.952921</td>\n",
              "      <td>0.902727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2659' max='2659' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2659/2659 02:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=3e-05, BS=32, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:41, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>0.228084</td>\n",
              "      <td>0.897577</td>\n",
              "      <td>0.857729</td>\n",
              "      <td>0.951261</td>\n",
              "      <td>0.902077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.227206</td>\n",
              "      <td>0.899117</td>\n",
              "      <td>0.858035</td>\n",
              "      <td>0.954509</td>\n",
              "      <td>0.903704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1330/1330 01:52]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'learning_rate': 3e-05, 'batch_size': 32}\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "learning_rates = [2e-5, 3e-5]\n",
        "batch_sizes = [16, 32]\n",
        "\n",
        "best_params = hyperparameter_tuning(train_data, val_data, tokenizer, learning_rates, batch_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T09:05:47.572872Z",
          "iopub.status.busy": "2024-12-23T09:05:47.572520Z",
          "iopub.status.idle": "2024-12-23T10:09:27.274600Z",
          "shell.execute_reply": "2024-12-23T10:09:27.273853Z",
          "shell.execute_reply.started": "2024-12-23T09:05:47.572830Z"
        },
        "id": "Iqho6Cr2xzTc",
        "outputId": "9e1243d1-e2c6-4998-c96a-23a1e08bd930",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:38, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>0.228084</td>\n",
              "      <td>0.897577</td>\n",
              "      <td>0.857729</td>\n",
              "      <td>0.951261</td>\n",
              "      <td>0.902077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.227206</td>\n",
              "      <td>0.899117</td>\n",
              "      <td>0.858035</td>\n",
              "      <td>0.954509</td>\n",
              "      <td>0.903704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14178, training_loss=0.23168202754152664, metrics={'train_runtime': 3818.8602, 'train_samples_per_second': 237.581, 'train_steps_per_second': 3.713, 'total_flos': 1.5023260148975616e+16, 'train_loss': 0.23168202754152664, 'epoch': 2.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_args = create_training_args(\n",
        "    output_dir=\"/content/drive/MyDrive/omdena_hackathon/models/malicious_url_detector/final_results\",\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    epochs=2,\n",
        "    run_name=\"malicious_url_detector_final\"\n",
        ")\n",
        "\n",
        "model = load_model()\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=final_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T10:09:27.275760Z",
          "iopub.status.busy": "2024-12-23T10:09:27.275455Z",
          "iopub.status.idle": "2024-12-23T10:10:04.961197Z",
          "shell.execute_reply": "2024-12-23T10:10:04.960445Z",
          "shell.execute_reply.started": "2024-12-23T10:09:27.275730Z"
        },
        "id": "74iB45L1xzTc",
        "outputId": "a6792601-4cd7-4cde-d9cb-41b5a18214f5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [444/444 00:37]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluation Metrics: {'eval_loss': 0.23061107099056244, 'eval_accuracy': 0.8962367297993158, 'eval_precision': 0.8554451710261569, 'eval_recall': 0.9548708590679393, 'eval_f1': 0.9024276996550809, 'eval_runtime': 37.6729, 'eval_samples_per_second': 752.61, 'eval_steps_per_second': 11.786, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.23061107099056244,\n",
              " 'eval_accuracy': 0.8962367297993158,\n",
              " 'eval_precision': 0.8554451710261569,\n",
              " 'eval_recall': 0.9548708590679393,\n",
              " 'eval_f1': 0.9024276996550809,\n",
              " 'eval_runtime': 37.6729,\n",
              " 'eval_samples_per_second': 752.61,\n",
              " 'eval_steps_per_second': 11.786,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model(trainer, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT97OBIexzTc"
      },
      "source": [
        "# thoughts\n",
        "\n",
        "This model performs really well in detecting malicious URLs. Its high recall and strong F1 score show it's excellent at catching malicious URLs with great accuracy. Plus, itâ€™s efficient, making it well-suited for real-world applications where speed is key. This model is showing a lot of promise and effectiveness."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
