{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrns1QdmxzTX"
      },
      "source": [
        "# Malicious URL Detection\n",
        "\n",
        "Project by:  \n",
        "[Jen Patrick Nataba](https://ph.linkedin.com/in/cytojen)  \n",
        "[John Ferry Lagman](https://ph.linkedin.com/in/thatjohnlagman)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JQacmY6yqgy",
        "outputId": "c3f0e735-9d0d-44e2-ccc6-a09a8b635a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:41.376344Z",
          "iopub.status.busy": "2024-12-23T04:07:41.376051Z",
          "iopub.status.idle": "2024-12-23T04:07:46.076673Z",
          "shell.execute_reply": "2024-12-23T04:07:46.075684Z",
          "shell.execute_reply.started": "2024-12-23T04:07:41.376314Z"
        },
        "id": "eSzNNZMexzTY",
        "outputId": "01e10b05-5aea-49ec-aa23-c80c9e04d6de",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid, fd = os.forkpty()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qL1ISnUxzTZ"
      },
      "source": [
        "# import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.077970Z",
          "iopub.status.busy": "2024-12-23T04:07:46.077721Z",
          "iopub.status.idle": "2024-12-23T04:07:46.082990Z",
          "shell.execute_reply": "2024-12-23T04:07:46.082085Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.077946Z"
        },
        "id": "qZdD3igFxzTZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beQOLiIAxzTZ"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.084131Z",
          "iopub.status.busy": "2024-12-23T04:07:46.083804Z",
          "iopub.status.idle": "2024-12-23T04:07:46.098933Z",
          "shell.execute_reply": "2024-12-23T04:07:46.098091Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.084103Z"
        },
        "id": "nHU_tMtfxzTZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "def load_data():\n",
        "    return load_dataset(\"kmack/Phishing_urls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.100019Z",
          "iopub.status.busy": "2024-12-23T04:07:46.099662Z",
          "iopub.status.idle": "2024-12-23T04:07:46.111589Z",
          "shell.execute_reply": "2024-12-23T04:07:46.110803Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.099934Z"
        },
        "id": "7d_vL6GsxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "def preprocess_data(dataset):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def preprocess(example):\n",
        "        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
        "\n",
        "    encoded_dataset = dataset.map(preprocess, batched=True, num_proc=4)\n",
        "    encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    return encoded_dataset, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.113007Z",
          "iopub.status.busy": "2024-12-23T04:07:46.112606Z",
          "iopub.status.idle": "2024-12-23T04:07:46.125346Z",
          "shell.execute_reply": "2024-12-23T04:07:46.124600Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.112974Z"
        },
        "id": "ZpAV3DqSxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# split into training, validation, and test sets\n",
        "def split_data(dataset):\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "    val_test_split = train_test_split['test'].train_test_split(test_size=0.25)\n",
        "\n",
        "    train_data = train_test_split['train']\n",
        "    val_data = val_test_split['train']\n",
        "    test_data = val_test_split['test']\n",
        "\n",
        "    return train_data, val_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPfapjGxzTa"
      },
      "source": [
        "# modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.127959Z",
          "iopub.status.busy": "2024-12-23T04:07:46.127741Z",
          "iopub.status.idle": "2024-12-23T04:07:46.137897Z",
          "shell.execute_reply": "2024-12-23T04:07:46.137129Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.127940Z"
        },
        "id": "j8U6dpzmxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# load distilbert model sequence classification\n",
        "def load_model(num_labels=2):\n",
        "    return DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=num_labels\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.146300Z",
          "iopub.status.busy": "2024-12-23T04:07:46.146088Z",
          "iopub.status.idle": "2024-12-23T04:07:46.161758Z",
          "shell.execute_reply": "2024-12-23T04:07:46.160956Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.146281Z"
        },
        "id": "rCFs0fYzxzTa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# accuracy, precision, recall, and F1-score\n",
        "def create_training_args(output_dir, learning_rate=2e-5, batch_size=16, epochs=2, run_name=\"malicious_url_detector\"):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        logging_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_steps=50,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=1,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        run_name=run_name,\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_strategy=\"epoch\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.163037Z",
          "iopub.status.busy": "2024-12-23T04:07:46.162705Z",
          "iopub.status.idle": "2024-12-23T04:07:46.174611Z",
          "shell.execute_reply": "2024-12-23T04:07:46.173914Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.162988Z"
        },
        "id": "wEZsK7oVxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZNbMzlWxzTb"
      },
      "source": [
        "# hyperparemeter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.175417Z",
          "iopub.status.busy": "2024-12-23T04:07:46.175226Z",
          "iopub.status.idle": "2024-12-23T04:07:46.183683Z",
          "shell.execute_reply": "2024-12-23T04:07:46.182912Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.175400Z"
        },
        "id": "fPLF6cvuxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning for learning rate, batch size, and epochs\n",
        "def hyperparameter_tuning(train_data, val_data, tokenizer, learning_rates, batch_sizes):\n",
        "    best_params = {}\n",
        "    best_f1 = 0\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"Training with LR={lr}, BS={bs}, Epochs=2\")\n",
        "\n",
        "            model = load_model()\n",
        "            args = create_training_args(\n",
        "                output_dir=f\"/content/drive/MyDrive/omdena_hackathon/models/malicious_url_detector\",\n",
        "                learning_rate=lr,\n",
        "                batch_size=bs,\n",
        "                epochs=2,\n",
        "                run_name=f\"LR_{lr}_BS_{bs}_E_2\"\n",
        "            )\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=train_data,\n",
        "                eval_dataset=val_data,\n",
        "                tokenizer=tokenizer,\n",
        "                compute_metrics=compute_metrics\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "            metrics = trainer.evaluate(val_data)\n",
        "            f1 = metrics[\"eval_f1\"]\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_params = {\"learning_rate\": lr, \"batch_size\": bs}\n",
        "\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_BOsIBNxzTb"
      },
      "source": [
        "# performance function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.184657Z",
          "iopub.status.busy": "2024-12-23T04:07:46.184414Z",
          "iopub.status.idle": "2024-12-23T04:07:46.199379Z",
          "shell.execute_reply": "2024-12-23T04:07:46.198610Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.184639Z"
        },
        "id": "dlFHl_SVxzTb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# check model performance\n",
        "def evaluate_model(trainer, test_data):\n",
        "    metrics = trainer.evaluate(test_data)\n",
        "    print(\"evaluation Metrics:\", metrics)\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOP_rC4cxzTb"
      },
      "source": [
        "# call the functions and run the codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3b4566e5f7dd422f952402d9750076ed",
            "55551a53650b4bdcaa90503f53c250e5",
            "3072825ff8d4436e881714c731960def",
            "3df96e6178ec4409a6e079b6101b579d",
            "189d59d36c4b40e8a39aa34083224659",
            "50c550cb4a6e4d9ea2e5a002a7bac492",
            "4d9ae48f8d144c87bd07888c9c298cbb",
            "75c004df55e0476c95d570b18e421678",
            "c0e748cdb0d945b7906e5f3117831f3f",
            "a4403d9283704a018d72fa1bdb05a78a",
            "a2ea81e17fb94da1bc79c7880c4437e5",
            "be0c4b6a16c8476b84649d6b7c8d71d0",
            "d0f417bbd89047f7a44cc3d6bce5174a",
            "dec28026f55e48cea45a78eb4d9f9835"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-23T04:07:46.200392Z",
          "iopub.status.busy": "2024-12-23T04:07:46.200176Z",
          "iopub.status.idle": "2024-12-23T04:09:42.452036Z",
          "shell.execute_reply": "2024-12-23T04:09:42.451030Z",
          "shell.execute_reply.started": "2024-12-23T04:07:46.200374Z"
        },
        "id": "spCagm_VxzTb",
        "outputId": "cc4cae33-37a7-41e5-abaf-7c94b491a912",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b4566e5f7dd422f952402d9750076ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/518 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55551a53650b4bdcaa90503f53c250e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-d8afc95a165ea87b.parquet:   0%|          | 0.00/25.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3072825ff8d4436e881714c731960def",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-4d6cbda5297196e7.parquet:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3df96e6178ec4409a6e079b6101b579d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-4e1abfe96aa382c2.parquet:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "189d59d36c4b40e8a39aa34083224659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/567056 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50c550cb4a6e4d9ea2e5a002a7bac492",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d9ae48f8d144c87bd07888c9c298cbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating valid split:   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75c004df55e0476c95d570b18e421678",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0e748cdb0d945b7906e5f3117831f3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4403d9283704a018d72fa1bdb05a78a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2ea81e17fb94da1bc79c7880c4437e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be0c4b6a16c8476b84649d6b7c8d71d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/567056 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0f417bbd89047f7a44cc3d6bce5174a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dec28026f55e48cea45a78eb4d9f9835",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/70882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_data()\n",
        "encoded_dataset, tokenizer = preprocess_data(dataset)\n",
        "train_data, val_data, test_data = split_data(encoded_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7dfc0d68588b4b75b3562b8c7fc5764f"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-23T04:09:42.453370Z",
          "iopub.status.busy": "2024-12-23T04:09:42.453116Z",
          "iopub.status.idle": "2024-12-23T09:05:47.571271Z",
          "shell.execute_reply": "2024-12-23T09:05:47.570339Z",
          "shell.execute_reply.started": "2024-12-23T04:09:42.453337Z"
        },
        "id": "OvwJ6GOTxzTb",
        "outputId": "480406fa-9118-4f1b-a859-81d318c4ad50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=2e-05, BS=16, Epochs=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dfc0d68588b4b75b3562b8c7fc5764f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28354' max='28354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28354/28354 1:19:56, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.257200</td>\n",
              "      <td>0.235134</td>\n",
              "      <td>0.896860</td>\n",
              "      <td>0.857865</td>\n",
              "      <td>0.949317</td>\n",
              "      <td>0.901277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.203900</td>\n",
              "      <td>0.234728</td>\n",
              "      <td>0.898647</td>\n",
              "      <td>0.857960</td>\n",
              "      <td>0.953489</td>\n",
              "      <td>0.903206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2659' max='2659' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2659/2659 02:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=2e-05, BS=32, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:43, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.230400</td>\n",
              "      <td>0.230629</td>\n",
              "      <td>0.896766</td>\n",
              "      <td>0.859242</td>\n",
              "      <td>0.946970</td>\n",
              "      <td>0.900975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.202100</td>\n",
              "      <td>0.226526</td>\n",
              "      <td>0.898941</td>\n",
              "      <td>0.858402</td>\n",
              "      <td>0.953513</td>\n",
              "      <td>0.903461</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1330/1330 01:53]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=3e-05, BS=16, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28354' max='28354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28354/28354 1:20:04, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.236194</td>\n",
              "      <td>0.896719</td>\n",
              "      <td>0.855801</td>\n",
              "      <td>0.952186</td>\n",
              "      <td>0.901424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.206200</td>\n",
              "      <td>0.238984</td>\n",
              "      <td>0.898153</td>\n",
              "      <td>0.857557</td>\n",
              "      <td>0.952921</td>\n",
              "      <td>0.902727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2659' max='2659' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2659/2659 02:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR=3e-05, BS=32, Epochs=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:41, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>0.228084</td>\n",
              "      <td>0.897577</td>\n",
              "      <td>0.857729</td>\n",
              "      <td>0.951261</td>\n",
              "      <td>0.902077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.227206</td>\n",
              "      <td>0.899117</td>\n",
              "      <td>0.858035</td>\n",
              "      <td>0.954509</td>\n",
              "      <td>0.903704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1330/1330 01:52]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'learning_rate': 3e-05, 'batch_size': 32}\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "learning_rates = [2e-5, 3e-5]\n",
        "batch_sizes = [16, 32]\n",
        "\n",
        "best_params = hyperparameter_tuning(train_data, val_data, tokenizer, learning_rates, batch_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T09:05:47.572872Z",
          "iopub.status.busy": "2024-12-23T09:05:47.572520Z",
          "iopub.status.idle": "2024-12-23T10:09:27.274600Z",
          "shell.execute_reply": "2024-12-23T10:09:27.273853Z",
          "shell.execute_reply.started": "2024-12-23T09:05:47.572830Z"
        },
        "id": "Iqho6Cr2xzTc",
        "outputId": "9e1243d1-e2c6-4998-c96a-23a1e08bd930",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14178' max='14178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14178/14178 1:03:38, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228300</td>\n",
              "      <td>0.228084</td>\n",
              "      <td>0.897577</td>\n",
              "      <td>0.857729</td>\n",
              "      <td>0.951261</td>\n",
              "      <td>0.902077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.227206</td>\n",
              "      <td>0.899117</td>\n",
              "      <td>0.858035</td>\n",
              "      <td>0.954509</td>\n",
              "      <td>0.903704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14178, training_loss=0.23168202754152664, metrics={'train_runtime': 3818.8602, 'train_samples_per_second': 237.581, 'train_steps_per_second': 3.713, 'total_flos': 1.5023260148975616e+16, 'train_loss': 0.23168202754152664, 'epoch': 2.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_args = create_training_args(\n",
        "    output_dir=\"/content/drive/MyDrive/omdena_hackathon/models/malicious_url_detector/final_results\",\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    epochs=2,\n",
        "    run_name=\"malicious_url_detector_final\"\n",
        ")\n",
        "\n",
        "model = load_model()\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=final_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-23T10:09:27.275760Z",
          "iopub.status.busy": "2024-12-23T10:09:27.275455Z",
          "iopub.status.idle": "2024-12-23T10:10:04.961197Z",
          "shell.execute_reply": "2024-12-23T10:10:04.960445Z",
          "shell.execute_reply.started": "2024-12-23T10:09:27.275730Z"
        },
        "id": "74iB45L1xzTc",
        "outputId": "a6792601-4cd7-4cde-d9cb-41b5a18214f5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [444/444 00:37]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluation Metrics: {'eval_loss': 0.23061107099056244, 'eval_accuracy': 0.8962367297993158, 'eval_precision': 0.8554451710261569, 'eval_recall': 0.9548708590679393, 'eval_f1': 0.9024276996550809, 'eval_runtime': 37.6729, 'eval_samples_per_second': 752.61, 'eval_steps_per_second': 11.786, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.23061107099056244,\n",
              " 'eval_accuracy': 0.8962367297993158,\n",
              " 'eval_precision': 0.8554451710261569,\n",
              " 'eval_recall': 0.9548708590679393,\n",
              " 'eval_f1': 0.9024276996550809,\n",
              " 'eval_runtime': 37.6729,\n",
              " 'eval_samples_per_second': 752.61,\n",
              " 'eval_steps_per_second': 11.786,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model(trainer, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT97OBIexzTc"
      },
      "source": [
        "# thoughts\n",
        "\n",
        "This model performs really well in detecting malicious URLs. Its high recall and strong F1 score show it's excellent at catching malicious URLs with great accuracy. Plus, it’s efficient, making it well-suited for real-world applications where speed is key. This model is showing a lot of promise and effectiveness."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
